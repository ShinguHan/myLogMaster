# shinguhan/mylogmaster/myLogMaster-main/app_controller.py

import pandas as pd
import json
import os, re
from PySide6.QtCore import QObject, Signal, QDateTime, QTimer

from universal_parser import parse_log_with_profile
from models.LogTableModel import LogTableModel
from analysis_result import AnalysisResult
from database_manager import DatabaseManager
from oracle_fetcher import OracleFetcherThread

CONFIG_FILE = 'config.json' # ğŸ’¥ ì„¤ì • íŒŒì¼ ê²½ë¡œë¥¼ ìƒìˆ˜ë¡œ ì •ì˜
FILTERS_FILE = 'filters.json'
SCENARIOS_DIR = 'scenarios'
HIGHLIGHTERS_FILE = 'highlighters.json' # âœ… íŒŒì¼ ê²½ë¡œ ì¶”ê°€

class AppController(QObject):
    model_updated = Signal(LogTableModel)
    fetch_completed = Signal()
    fetch_progress = Signal(str)
        # âœ… í˜„ì¬ í–‰ ê°œìˆ˜ë¥¼ ì „ë‹¬í•  ìƒˆë¡œìš´ ì‹ í˜¸
    row_count_updated = Signal(int)
        # âœ… 1. UIì— ì—ëŸ¬ë¥¼ ì „ë‹¬í•  ìƒˆë¡œìš´ ì‹ í˜¸
    fetch_error = Signal(str)

    def __init__(self, app_mode, connection_name=None, connection_info=None):
        super().__init__()
        self.mode = app_mode
        self.connection_name = connection_name
        self.connection_info = connection_info
        self.original_data = pd.DataFrame()
        # âœ… 1. ëª¨ë¸ ìƒì„± ì‹œ ìµœëŒ€ í–‰ ìˆ˜ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤. (í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 20000ìœ¼ë¡œ ì„¤ì •)
        self.source_model = LogTableModel(max_rows=20000) 
        self.fetch_thread = None
        self.last_query_conditions = None # âœ… ì¡°íšŒ ì¡°ê±´ ì €ì¥í•  ë³€ìˆ˜ ì¶”ê°€
                # âœ… 1. ëŒ€ì‹œë³´ë“œ ë‹¤ì´ì–¼ë¡œê·¸ ì°¸ì¡°ë¥¼ ì €ì¥í•  ë³€ìˆ˜ ì¶”ê°€
        self.dashboard_dialog = None 
        self.current_theme = 'light' # âœ… 1. í˜„ì¬ í…Œë§ˆ ì €ì¥ ë³€ìˆ˜

                # âœ… 1. ë°ì´í„° ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ íì™€ íƒ€ì´ë¨¸ ì¶”ê°€
        self._update_queue = []
        self._update_timer = QTimer(self)
        self._update_timer.setInterval(200) # 200ms (0.2ì´ˆ) ë§ˆë‹¤ ì‹¤í–‰
        self._update_timer.timeout.connect(self._process_update_queue)

        self.config = self._load_config() # ğŸ’¥ __init__ì—ì„œ ì „ì²´ ì„¤ì •ì„ ë¡œë“œ
        self.highlighting_rules = self.load_highlighting_rules() # âœ… ì‹œì‘ ì‹œ ê·œì¹™ ë¡œë“œ

        if self.mode == 'realtime':
            if self.connection_name:
                self.db_manager = DatabaseManager(self.connection_name)
                self.load_data_from_cache()
            else:
                print("Error: Realtime mode requires a connection name.")
                self.db_manager = None
        else:
            # íŒŒì¼ ëª¨ë“œì—ì„œëŠ” 'file_mode'ë¼ëŠ” ê³ ì •ëœ ì´ë¦„ìœ¼ë¡œ DB ìƒì„±
            self.db_manager = DatabaseManager("file_mode")

    # ğŸ’¥ ë³€ê²½ì  1: í…Œë§ˆ ê´€ë ¨ ë©”ì†Œë“œ ì „ì²´ë¥¼ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•©ë‹ˆë‹¤.
    def _load_config(self):
        """
        ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ config.jsonì—ì„œ ì„¤ì •ì„ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
        íŒŒì¼ì´ ì—†ê±°ë‚˜ ì†ìƒëœ ê²½ìš° ê¸°ë³¸ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        default_config = {'theme': 'light', 'visible_columns': []}
        if not os.path.exists(CONFIG_FILE):
            return default_config
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, Exception):
            print(f"Warning: Could not read {CONFIG_FILE}. Using default config.")
            return default_config
        
    def save_config(self):
        """í˜„ì¬ ì„¤ì •(self.config)ì„ config.json íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=4, ensure_ascii=False)
            print("Configuration saved successfully.")
        except Exception as e:
            print(f"Error saving configuration: {e}")
            
    def load_data_from_cache(self):
        """ë¡œì»¬ ìºì‹œì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , ë°ì´í„° ìœ ë¬´ì™€ ìƒê´€ì—†ì´ í•­ìƒ UIì— ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ë„ë¡ ì‹ í˜¸ë¥¼ ë³´ëƒ…ë‹ˆë‹¤."""
        if not self.db_manager: 
            self.update_model_data(pd.DataFrame()) # DB ë§¤ë‹ˆì €ê°€ ì—†ì„ ë•Œë„ ì´ˆê¸° ëª¨ë¸ ì „ì†¡
            return
        
        print("Loading initial data from local cache...")
        cached_data = self.db_manager.read_all_logs_from_cache()
        
        if not cached_data.empty:
            self.original_data = cached_data
            if 'SystemDate' in self.original_data.columns and 'SystemDate_dt' not in self.original_data.columns:
                 self.original_data['SystemDate_dt'] = pd.to_datetime(self.original_data['SystemDate'], format='%d-%b-%Y %H:%M:%S:%f', errors='coerce')
            print(f"Loaded {len(cached_data)} rows from cache.")
        else:
            self.original_data = pd.DataFrame() # ìºì‹œê°€ ë¹„ì—ˆìœ¼ë©´ ë¹ˆ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì´ˆê¸°í™”
            print("Local cache is empty.")
        
        # âœ… ë°ì´í„°ê°€ ìˆë“  ì—†ë“ , ë¬´ì¡°ê±´ UIì— ëª¨ë¸ì„ ì„¤ì •í•˜ë¼ëŠ” ì‹ í˜¸ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
        self.update_model_data(self.original_data)

    
    def load_log_file(self, filepath):
        try:
            parsed_data = parse_log_with_profile(filepath, self._get_profile())
            self.original_data = pd.DataFrame(parsed_data)
            
            if 'SystemDate' in self.original_data.columns:
                self.original_data['SystemDate_dt'] = pd.to_datetime(
                    self.original_data['SystemDate'], format='%d-%b-%Y %H:%M:%S:%f', errors='coerce'
                )
            
            self.update_model_data(self.original_data)
            return not self.original_data.empty
        except Exception as e:
            print(f"Error loading or parsing file: {e}")
            self.original_data = pd.DataFrame()
            self.update_model_data(self.original_data)
            return False

    def _get_profile(self):
        return {
            'column_mapping': {'Category': 'Category', 'AsciiData': 'AsciiData', 'BinaryData': 'BinaryData'},
            'type_rules': [{'value': 'Com', 'type': 'secs'}, {'value': 'Info', 'type': 'json'}]
        }
    def update_model_data(self, dataframe):
        self.source_model.update_data(dataframe)
        # âœ… ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ í•˜ì´ë¼ì´íŠ¸ ê·œì¹™ë„ í•¨ê»˜ ì ìš©
        self.source_model.set_highlighting_rules(self.highlighting_rules)
        self.model_updated.emit(self.source_model)

    # shinguhan/mylogmaster/myLogMaster-32ac0fde18fc7e07d5e70cee360ee943bd9507be/app_controller.py

    # shinguhan/mylogmaster/myLogMaster-32ac0fde18fc7e07d5e70cee360ee943bd9507be/app_controller.py

    def start_db_fetch(self, query_conditions):
        if self.fetch_thread and self.fetch_thread.isRunning():
            print("Fetch is already in progress.")
            return

        self.last_query_conditions = query_conditions
        
        # [ìˆ˜ì •] WHERE ì ˆì„ ë¯¸ë¦¬ ë§Œë“¤ì§€ ì•Šê³ , ì¡°ê±´ ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
        # where_clause, params = self._build_where_clause(query_conditions)

        if self.db_manager:
            self.db_manager.clear_logs_from_cache()

        self.source_model.update_data(pd.DataFrame())
        self.original_data = pd.DataFrame()

        # [ìˆ˜ì •] OracleFetcherThreadì— ì¡°ê±´ ê°ì²´ë¥¼ í†µì§¸ë¡œ ì „ë‹¬í•˜ê³ , selfë¥¼ parentë¡œ ì§€ì •í•©ë‹ˆë‹¤.
        self.fetch_thread = OracleFetcherThread(self.connection_info, query_conditions, parent=self)
        
        self.fetch_thread.data_fetched.connect(self.append_data_chunk)
        self.fetch_thread.finished.connect(self.on_fetch_finished)
        self.fetch_thread.progress.connect(self.fetch_progress)
        self.fetch_thread.error.connect(self._handle_fetch_error)

        if self.dashboard_dialog and self.dashboard_dialog.isVisible():
            self.dashboard_dialog.start_updates()

        self._update_timer.start()
        self.fetch_thread.start()


    # âœ… 2. append_data_chunk ë©”ì†Œë“œì— original_data ì˜ë¼ë‚´ëŠ” ë¡œì§ ì¶”ê°€
    def append_data_chunk(self, df_chunk):
        """ë°›ì€ ë°ì´í„° ì¡°ê°ì„ ë°”ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šê³  íì— ì¶”ê°€í•©ë‹ˆë‹¤."""
        if not df_chunk.empty:
            self._update_queue.append(df_chunk)

    def on_fetch_finished(self):
        """ë°ì´í„° ìˆ˜ì‹ ì´ ì™„ë£Œ/ì¤‘ë‹¨/ì—ëŸ¬ ë°œìƒ í›„ ê³µí†µìœ¼ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤."""
        # ë§ˆì§€ë§‰ ë‚¨ì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬
        if self._update_queue:
            self._process_update_queue()
        
        print("Fetch thread finished.")
        if self._update_timer.isActive():
            self._update_timer.stop()

        # âœ… 3. ì‘ì—… ì™„ë£Œ ì‹œì—ë„ ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì¤‘ì§€ ëª…ë ¹
        if self.dashboard_dialog and self.dashboard_dialog.isVisible():
            self.dashboard_dialog.stop_updates()
            
        self.fetch_completed.emit()

        # âœ… ë°ì´í„° ì¡°íšŒê°€ ì„±ê³µì ìœ¼ë¡œ ëë‚¬ì„ ë•Œë§Œ ì´ë ¥ì„ ì €ì¥í•©ë‹ˆë‹¤.
        if self.db_manager and self.last_query_conditions:
            start_time = self.last_query_conditions.get('start_time', '')
            end_time = self.last_query_conditions.get('end_time', '')
            # start_time, end_timeì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì¡°ê±´ì„ filtersë¡œ ê°„ì£¼
            filters = {k: v for k, v in self.last_query_conditions.items() if k not in ['start_time', 'end_time']}
            self.db_manager.add_fetch_history(start_time, end_time, filters)
        
    # ... (load_log_file, run_analysis_script ë“± ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼)     
    def run_analysis_script(self, script_code, dataframe):
        try:
            # ì—¬ê¸°ì— ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë¡œì§ì„ êµ¬í˜„í•˜ì„¸ìš”
            # ì„ì‹œë¡œ ê¸°ë³¸ AnalysisResult ë°˜í™˜
            return AnalysisResult()
        except Exception as e:
            print(f"Script execution error: {e}")
            return AnalysisResult()

    def clear_advanced_filter(self):
        self.update_model_data(self.original_data)

    def apply_advanced_filter(self, query_data):
        if not query_data or not query_data.get('rules'):
            self.clear_advanced_filter()
            return
            
        if self.original_data.empty:
            return

        try:
            final_mask = self._build_mask_recursive(query_data, self.original_data)
            self.update_model_data(self.original_data[final_mask])
        except Exception as e:
            print(f"Error applying filter: {e}")
            self.update_model_data(self.original_data)

    def _build_mask_recursive(self, query_group, df):
        masks = []
        for rule in query_group.get('rules', []):
            if 'logic' in rule: 
                masks.append(self._build_mask_recursive(rule, df))
            else: 
                column, op, value = rule['column'], rule['operator'], rule['value']
                if not all([column, op]): continue

                mask = pd.Series(True, index=df.index)
                try:
                    if column == 'SystemDate':
                        dt_value = pd.to_datetime(value) if not isinstance(value, tuple) else None
                        dt_value_from = pd.to_datetime(value[0]) if isinstance(value, tuple) else None
                        dt_value_to = pd.to_datetime(value[1]) if isinstance(value, tuple) else None
                        
                        if op == 'is after': mask = df[f'{column}_dt'] > dt_value
                        elif op == 'is before': mask = df[f'{column}_dt'] < dt_value
                        elif op == 'is between': mask = (df[f'{column}_dt'] >= dt_value_from) & (df[f'{column}_dt'] <= dt_value_to)
                    else:
                        series = df[column].astype(str)
                        if op == 'Contains': mask = series.str.contains(value, case=False, na=False)
                        elif op == 'Does Not Contain': mask = ~series.str.contains(value, case=False, na=False)
                        elif op == 'Equals': mask = series == value
                        elif op == 'Not Equals': mask = series != value
                        elif op == 'Matches Regex': mask = series.str.match(value, na=False)
                    masks.append(mask)
                except KeyError:
                    print(f"Warning: Column '{column}' not found for filtering. Skipping rule.")
                    continue

        if not masks:
            return pd.Series(True, index=df.index)

        if query_group['logic'] == 'AND':
            return pd.concat(masks, axis=1).all(axis=1)
        else:
            return pd.concat(masks, axis=1).any(axis=1)

    def load_filters(self):
        try:
            if not os.path.exists(FILTERS_FILE): return {}
            with open(FILTERS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError:
            print(f"Warning: Could not parse '{FILTERS_FILE}'. It may be corrupted.")
            return {}
        except Exception as e:
            print(f"Error loading filters: {e}")
            return {}

    def save_filter(self, name, query_data):
        try:
            filters = self.load_filters()
            if query_data:
                filters[name] = query_data
                with open(FILTERS_FILE, 'w', encoding='utf-8') as f: 
                    json.dump(filters, f, indent=4)
        except Exception as e:
            print(f"Error saving filter '{name}': {e}")
    
    def get_trace_data(self, trace_id):
        df = self.original_data
        mask = df.apply(lambda r: r.astype(str).str.contains(trace_id, case=False, na=False).any(), axis=1)
        return df[mask]

    def get_scenario_data(self, trace_id):
        scenario_df = self.get_trace_data(trace_id)
        com_logs = scenario_df[scenario_df['Category'].str.replace('"', '', regex=False) == 'Com'].sort_values(by='SystemDate')
        return com_logs

    # shinguhan/mylogmaster/myLogMaster-main/app_controller.py

    # shinguhan/mylogmaster/myLogMaster-main/app_controller.py

    # shinguhan/mylogmaster/myLogMaster-main/app_controller.py

    def run_scenario_validation(self, scenario_to_run=None):
        if self.original_data.empty:
            return [] # í…ìŠ¤íŠ¸ ëŒ€ì‹  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        scenarios = self.load_all_scenarios()
        if "Error" in scenarios:
            return [{"scenario_name": "Error", "status": "FAIL", "message": scenarios['Error']['description']}]

        all_completed_scenarios = []
        df = self.original_data.sort_values(by='SystemDate_dt').reset_index()

        for name, scenario in scenarios.items():
            if (scenario_to_run and name != scenario_to_run) or \
               (scenario_to_run is None and not scenario.get("enabled", True)):
                continue

            active_scenarios = {}
            completed_scenarios = []
            context_keys = list(scenario.get("context_extractors", {}).keys())

            for index, row in df.iterrows():
                current_time = row['SystemDate_dt']
                current_row_context = self._extract_context(row, scenario.get("context_extractors", {}))

                # --- 1. íƒ€ì„ì•„ì›ƒ ë° ì´ë²¤íŠ¸ ë§¤ì¹­ ê²€ì‚¬ ---
                finished_keys = []
                for key, state in active_scenarios.items():
                    if context_keys and any(k in current_row_context for k in context_keys):
                        context_match = all(state['context'].get(k) == current_row_context.get(k) for k in context_keys if k in current_row_context)
                        if not context_match: continue
                    
                    if state['current_step'] >= len(state['steps']): continue
                    step_definition = state['steps'][state['current_step']]
                    
                    # --- [ë³µì›] íƒ€ì„ì•„ì›ƒ ê²€ì‚¬ ---
                    if 'max_delay_seconds' in step_definition:
                        time_limit = pd.Timedelta(seconds=step_definition['max_delay_seconds'])
                        if current_time > state['last_event_time'] + time_limit:
                            if step_definition.get('optional', False):
                                state['current_step'] += 1
                                if state['current_step'] >= len(state['steps']):
                                    state['status'] = 'SUCCESS'; state['message'] = 'Completed (last optional step timed out).'; completed_scenarios.append(state); finished_keys.append(key)
                                continue
                            else:
                                state['status'] = 'FAIL'; state['message'] = f"Timeout at Step {state['current_step'] + 1}: {step_definition.get('name', 'N/A')}"; completed_scenarios.append(state); finished_keys.append(key)
                                continue
                    
                    # --- [ë³µì›] ì´ë²¤íŠ¸ ë§¤ì¹­ (Optional, Unordered ë“±) ---
                    step_matched = False
                    if step_definition.get('optional', False) and (state['current_step'] + 1) < len(state['steps']):
                        next_step_definition = state['steps'][state['current_step'] + 1]
                        if self._match_event(row, next_step_definition.get('event_match', {})):
                            state['current_step'] += 1; step_definition = next_step_definition
                    
                    if 'unordered_group' in step_definition:
                        found_event_name = None
                        for event_in_group in state.get('unordered_events', []):
                            if self._match_event(row, event_in_group['event_match']):
                                found_event_name = event_in_group['name']; break
                        if found_event_name:
                            state['unordered_events'] = [e for e in state['unordered_events'] if e['name'] != found_event_name]
                            if not state['unordered_events']: state['current_step'] += 1
                            step_matched = True
                    elif self._match_event(row, step_definition.get('event_match', {})):
                        state['current_step'] += 1
                        step_matched = True

                    # âœ… [ìˆ˜ì •] ë‹¨ê³„ ì„±ê³µ ì‹œ, ìƒì„¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ì €ì¥í•˜ë„ë¡ ë³€ê²½
                    if step_matched:
                        state['last_event_time'] = current_time
                        step_name = step_definition.get('name', f"Step {state['current_step']}")
                        state['involved_logs'].append({
                            "step_name": step_name,
                            "log_index": int(row['index']),
                            "timestamp": row['SystemDate_dt']
                        })

                    if state['current_step'] >= len(state['steps']):
                        state['status'] = 'SUCCESS'; state['message'] = 'Scenario completed successfully.'; completed_scenarios.append(state); finished_keys.append(key)

                for key in finished_keys: del active_scenarios[key]

                # --- 2. ìƒˆë¡œìš´ ì‹œë‚˜ë¦¬ì˜¤ ì‹œì‘(Trigger) ê²€ì‚¬ ---
                if self._match_event(row, scenario.get('trigger_event', {})):
                    trigger_context = self._extract_context(row, scenario.get("context_extractors", {}))
                    key = None
                    if context_keys:
                        if trigger_context and all(k in trigger_context for k in context_keys):
                            key = tuple(trigger_context[k] for k in context_keys)
                    else: key = row['index']

                    if key is not None and key not in active_scenarios:
                        new_state = {
                            'context': trigger_context,
                            'steps': list(scenario['steps']),
                            'start_time': current_time,
                            'last_event_time': current_time,
                            'current_step': 0,
                            'status': 'IN_PROGRESS',
                            'scenario_name': name,
                            # âœ… [ìˆ˜ì •] involved_logsì— ìƒì„¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ì €ì¥í•˜ë„ë¡ ë³€ê²½
                            'involved_logs': [{"step_name": "Trigger", "log_index": int(row['index']), "timestamp": row['SystemDate_dt']}]
                        }
                        if 'unordered_group' in new_state['steps'][0]:
                            new_state['unordered_events'] = list(new_state['steps'][0]['unordered_group'])
                        active_scenarios[key] = new_state
            
            # --- 3. ìµœì¢… ê²°ê³¼ ì²˜ë¦¬ ---
            for key, state in active_scenarios.items():
                state['status'] = 'INCOMPLETE'; state['message'] = f"Scenario stopped at step {state['current_step'] + 1}."; completed_scenarios.append(state)
            
            all_completed_scenarios.extend(completed_scenarios)

        if self.db_manager:
            for report in all_completed_scenarios:
                self.db_manager.add_validation_history(
                    scenario_name=report['scenario_name'],
                    status=report['status'],
                    message=report.get('message', ''),
                    involved_log_indices=report.get('involved_logs', [])
                )
        
        return all_completed_scenarios

    # shinguhan/mylogmaster/myLogMaster-main/app_controller.py

    def _match_event(self, row, rule_group):
        """
        ë‹¨ìˆœ ì¡°ê±´ê³¼ ë³µí•© ì¡°ê±´(AND/OR) ëª¨ë‘ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ”
        ìƒˆë¡­ê³  ì•ˆì •ì ì¸ ì´ë²¤íŠ¸ ë§¤ì¹­ í•¨ìˆ˜ì…ë‹ˆë‹¤.
        """
        if not rule_group:
            return False

        # 'logic' í‚¤ê°€ ìˆìœ¼ë©´ ë³µí•© ì¡°ê±´ ê·¸ë£¹ìœ¼ë¡œ ì²˜ë¦¬
        if "logic" in rule_group:
            logic = rule_group.get("logic", "AND").upper()
            
            # ëª¨ë“  í•˜ìœ„ ê·œì¹™(rules)ì— ëŒ€í•´ ì¬ê·€ì ìœ¼ë¡œ _match_event í˜¸ì¶œ
            for sub_rule in rule_group.get("rules", []):
                result = self._match_event(row, sub_rule)
                
                if logic == "AND" and not result:
                    return False # AND ê·¸ë£¹ì—ì„œëŠ” í•˜ë‚˜ë¼ë„ Falseì´ë©´ ì¦‰ì‹œ False
                if logic == "OR" and result:
                    return True # OR ê·¸ë£¹ì—ì„œëŠ” í•˜ë‚˜ë¼ë„ Trueì´ë©´ ì¦‰ì‹œ True
            
            # ë£¨í”„ê°€ ëë‚¬ì„ ë•Œì˜ ìµœì¢… ê²°ê³¼
            return True if logic == "AND" else False
        
        # 'logic' í‚¤ê°€ ì—†ìœ¼ë©´ ë‹¨ì¼ ì¡°ê±´ìœ¼ë¡œ ì²˜ë¦¬
        else:
            col = rule_group.get("column")
            op = rule_group.get("operator")
            val = rule_group.get("value")
            
            # ê·œì¹™ì˜ í•„ìˆ˜ ìš”ì†Œê°€ ì—†ê±°ë‚˜, ë¡œê·¸ì— í•´ë‹¹ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ False
            if not all([col, op, val is not None]) or col not in row.index or pd.isna(row[col]):
                return False
                
            cell_value = str(row[col]).lower()
            check_value = str(val).lower()
            
            if op == "contains":
                return check_value in cell_value
            elif op == "equals":
                return check_value == cell_value
            elif op == "starts with":
                return cell_value.startswith(check_value)
            elif op == "ends with":
                return cell_value.endswith(check_value)
            else:
                return False

    def load_all_scenarios(self):
        all_scenarios = {}
        if not os.path.exists(SCENARIOS_DIR):
            return {"Error": {"description": f"'{SCENARIOS_DIR}' directory not found."}}
        
        for filename in sorted(os.listdir(SCENARIOS_DIR)):
            if filename.endswith(".json"):
                filepath = os.path.join(SCENARIOS_DIR, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        scenarios_in_file = json.load(f)
                        # âœ… ê° ì‹œë‚˜ë¦¬ì˜¤ì— ì›ë³¸ íŒŒì¼ ì´ë¦„ì„ '_source_file' í‚¤ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
                        for name, details in scenarios_in_file.items():
                            details['_source_file'] = filename
                            all_scenarios[name] = details
                except (json.JSONDecodeError, Exception) as e:
                    print(f"Warning: Could not parse {filename}: {e}")
        return all_scenarios
    
    def get_scenario_names(self):
        scenarios = self.load_all_scenarios()
        # âœ… "enabled"ê°€ falseê°€ ì•„ë‹Œ ì‹œë‚˜ë¦¬ì˜¤ì˜ ì´ë¦„ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        return [name for name, details in scenarios.items() if details.get("enabled", True)]
    
    # âœ… 3. íƒ€ì´ë¨¸ê°€ í˜¸ì¶œí•  ìƒˆë¡œìš´ ë©”ì†Œë“œ
    def _process_update_queue(self):
        """íì— ìŒ“ì¸ ë°ì´í„° ì¡°ê°ë“¤ì„ í•œ ë²ˆì— ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        if not self._update_queue:
            return

        # íì— ìˆëŠ” ëª¨ë“  ë°ì´í„° ì¡°ê°ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
        combined_chunk = pd.concat(self._update_queue, ignore_index=True)
        self._update_queue.clear()

        print(f"Processing a combined chunk of {len(combined_chunk)} logs.")

        if 'SystemDate' in combined_chunk.columns and 'SystemDate_dt' not in combined_chunk.columns:
            combined_chunk['SystemDate_dt'] = pd.to_datetime(combined_chunk['SystemDate'], format='%d-%b-%Y %H:%M:%S:%f', errors='coerce')

        self.original_data = pd.concat([self.original_data, combined_chunk], ignore_index=True)
        self.source_model.append_data(combined_chunk)
        
        current_model_rows = self.source_model.rowCount()
        if len(self.original_data) > current_model_rows:
            self.original_data = self.original_data.tail(current_model_rows).reset_index(drop=True)

        if self.db_manager:
            self.db_manager.upsert_logs_to_local_cache(combined_chunk)
        
        # UIì— í˜„ì¬ ì´ í–‰ ìˆ˜ë¥¼ ì•Œë¦¼
        self.row_count_updated.emit(self.source_model.rowCount())

                # âœ… 2. ëŒ€ì‹œë³´ë“œê°€ ì—´ë ¤ìˆìœ¼ë©´, ì—…ë°ì´íŠ¸ ì‹ í˜¸ë¥¼ ë³´ëƒ„
        if self.dashboard_dialog and self.dashboard_dialog.isVisible():
            # ì „ì²´ original_dataë¥¼ ë„˜ê²¨ì£¼ì–´ ëŒ€ì‹œë³´ë“œê°€ í•­ìƒ ìµœì‹  ìƒíƒœë¥¼ ë°˜ì˜í•˜ê²Œ í•¨
            self.dashboard_dialog.update_dashboard(self.original_data)

        # âœ… 3. UIì˜ ì·¨ì†Œ ìš”ì²­ì„ ì²˜ë¦¬í•  ìƒˆë¡œìš´ ë©”ì†Œë“œ
    def cancel_db_fetch(self):
        """ì‹¤í–‰ ì¤‘ì¸ ë°ì´í„° fetch ìŠ¤ë ˆë“œë¥¼ ì¤‘ì§€ì‹œí‚µë‹ˆë‹¤."""
        if self.fetch_thread and self.fetch_thread.isRunning():
            self.fetch_thread.stop()
            # finished ì‹ í˜¸ê°€ ì˜¬ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³ , UIëŠ” ì¦‰ì‹œ ë°˜ì‘í•˜ë„ë¡ í•©ë‹ˆë‹¤.
            # ìŠ¤ë ˆë“œëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë©ˆì¶”ê³  on_fetch_finishedë¥¼ í˜¸ì¶œí•˜ì—¬ ì •ë¦¬í•  ê²ƒì…ë‹ˆë‹¤.
    
    # âœ… 4. ìŠ¤ë ˆë“œì˜ ì—ëŸ¬ë¥¼ ë°›ì•„ UIì— ì „ë‹¬í•˜ëŠ” ìƒˆë¡œìš´ ìŠ¬ë¡¯
    def _handle_fetch_error(self, error_message):
        """Fetcher ìŠ¤ë ˆë“œì—ì„œ ë°œìƒí•œ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        print(f"Controller caught error: {error_message}")
        self.fetch_error.emit(error_message)

               # âœ… 2. ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì¤‘ì§€ ëª…ë ¹
        if self.dashboard_dialog and self.dashboard_dialog.isVisible():
            self.dashboard_dialog.stop_updates()

        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ íƒ€ì´ë¨¸ëŠ” ì¤‘ì§€í•´ì•¼ í•©ë‹ˆë‹¤.
        if self._update_timer.isActive():
            self._update_timer.stop()

        # âœ… 2. í…Œë§ˆ ì„¤ì •ì„ ìœ„í•œ ìƒˆë¡œìš´ ë©”ì†Œë“œë“¤
    def set_current_theme(self, theme_name):
        """ë©”ëª¨ë¦¬ì— í˜„ì¬ í…Œë§ˆë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ , ì¦‰ì‹œ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        self.config['theme'] = theme_name
        self.save_config() # ğŸ’¥ í…Œë§ˆ ë³€ê²½ ì‹œ ì¦‰ì‹œ ì €ì¥

    def get_current_theme(self):
        """ë©”ëª¨ë¦¬ì— ì €ì¥ëœ í˜„ì¬ í…Œë§ˆë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.config.get('theme', 'light')
    
        # âœ… í•˜ì´ë¼ì´íŠ¸ ê·œì¹™ì„ ìœ„í•œ ìƒˆë¡œìš´ ë©”ì†Œë“œë“¤
    def load_highlighting_rules(self):
        if not os.path.exists(HIGHLIGHTERS_FILE):
            return []
        try:
            with open(HIGHLIGHTERS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, Exception):
            return []

    def apply_new_highlighting_rules(self):
        self.highlighting_rules = self.load_highlighting_rules()
        self.source_model.set_highlighting_rules(self.highlighting_rules)

        # âœ… ì•„ë˜ ë©”ì†Œë“œë¥¼ í´ë˜ìŠ¤ ë§¨ ëì— ì¶”ê°€í•´ì£¼ì„¸ìš”.
    def save_log_to_csv(self, dataframe, file_path):
        """ë°ì´í„°í”„ë ˆì„ì„ ì§€ì •ëœ ê²½ë¡œì˜ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # utf-8-sig ì¸ì½”ë”©ì€ Excelì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.
            dataframe.to_csv(file_path, index=False, encoding='utf-8-sig')
            return True, f"Successfully saved to {os.path.basename(file_path)}"
        except Exception as e:
            print(f"Error saving to CSV: {e}")
            return False, f"Could not save file: {e}"
        
    # âœ… 3. ì•„ë˜ì˜ ë‘ ë©”ì†Œë“œë¥¼ í´ë˜ìŠ¤ ë§¨ ëì— ìƒˆë¡œ ì¶”ê°€í•´ì£¼ì„¸ìš”.
    def _build_where_clause(self, query_conditions):
        """QueryConditionsDialogì—ì„œ ë°›ì€ ì¡°ê±´ìœ¼ë¡œ WHERE ì ˆê³¼ íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        clauses = []
        params = {}
        
        # ì‹œê°„ ì¡°ê±´ ì¶”ê°€ (Oracle DATE í˜•ì‹ì— ë§ê²Œ ë³€í™˜)
        params['p_start_time'] = pd.to_datetime(query_conditions['start_time']).strftime('%Y-%m-%d %H:%M:%S')
        params['p_end_time'] = pd.to_datetime(query_conditions['end_time']).strftime('%Y-%m-%d %H:%M:%S')
        clauses.append("SystemDate BETWEEN TO_DATE(:p_start_time, 'YYYY-MM-DD HH24:MI:SS') AND TO_DATE(:p_end_time, 'YYYY-MM-DD HH24:MI:SS')")
        
        # ê³ ê¸‰ í•„í„° ì¡°ê±´ ì¶”ê°€
        adv_filter = query_conditions.get('advanced_filter')
        if adv_filter and adv_filter.get('rules'):
            adv_clause, adv_params = self._parse_filter_group(adv_filter)
            if adv_clause:
                clauses.append(adv_clause)
                params.update(adv_params)
        
        return " AND ".join(clauses), params

    def _parse_filter_group(self, group, param_index=0):
        """ì¬ê·€ì ìœ¼ë¡œ í•„í„° ê·¸ë£¹ì„ íŒŒì‹±í•˜ì—¬ SQL ì¡°ê±´ë¬¸ê³¼ íŒŒë¼ë¯¸í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤."""
        clauses = []
        params = {}
        logic = f" {group.get('logic', 'AND')} "
        
        for rule in group.get('rules', []):
            if "logic" in rule: # í•˜ìœ„ ê·¸ë£¹ì¸ ê²½ìš°
                sub_clause, sub_params = self._parse_filter_group(rule, param_index)
                if sub_clause:
                    clauses.append(f"({sub_clause})")
                    params.update(sub_params)
                    param_index += len(sub_params)
            else: # ì‹¤ì œ ê·œì¹™ì¸ ê²½ìš°
                col = rule.get('column')
                op = rule.get('operator')
                val = rule.get('value')
                
                if not all([col, op, val]): continue
                
                param_name = f"p{param_index}"
                
                # ì—°ì‚°ìì— ë§ëŠ” SQL êµ¬ë¬¸ ìƒì„±
                if op == 'Contains':
                    clauses.append(f"INSTR({col}, :{param_name}) > 0")
                    params[param_name] = val
                elif op == 'Does Not Contain':
                    clauses.append(f"INSTR({col}, :{param_name}) = 0")
                    params[param_name] = val
                elif op == 'Equals':
                    clauses.append(f"{col} = :{param_name}")
                    params[param_name] = val
                elif op == 'Not Equals':
                    clauses.append(f"{col} != :{param_name}")
                    params[param_name] = val
                elif op == 'Matches Regex': # Oracle REGEXP_LIKE ì‚¬ìš©
                    clauses.append(f"REGEXP_LIKE({col}, :{param_name})")
                    params[param_name] = val

                param_index += 1
                
        return logic.join(clauses), params
    
    def _extract_context(self, row, extractors):
        """ì •ì˜ëœ ì—¬ëŸ¬ ì¶”ì¶œê¸° ê·œì¹™ì— ë”°ë¼ ë¡œê·¸(row)ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê°’ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤."""
        context_data = {}
        for context_name, rules in extractors.items():
            found_value = None
            for rule in rules:
                if "from_column" in rule:
                    val = row.get(rule["from_column"])
                    if pd.notna(val) and str(val).strip():
                        found_value = str(val)
                        break
                elif "from_regex" in rule:
                    source_col = rule["from_regex"].get("column")
                    pattern = rule["from_regex"].get("pattern")
                    source_text = str(row.get(source_col, ''))
                    match = re.search(pattern, source_text)
                    if match:
                        found_value = match.group(1) # ì²« ë²ˆì§¸ ê´„í˜¸ ê·¸ë£¹ì„ ì¶”ì¶œ
                        break
            if found_value:
                context_data[context_name] = found_value
        return context_data
    
        # âœ… ì•„ë˜ ë‘ ë©”ì†Œë“œë¥¼ í´ë˜ìŠ¤ ë§¨ ëì— ì¶”ê°€í•´ì£¼ì„¸ìš”.
    def get_history_summary(self):
        if self.db_manager:
            return self.db_manager.get_validation_history_summary()
        return pd.DataFrame()

    def get_history_detail(self, run_id):
        if self.db_manager:
            return self.db_manager.get_validation_history_detail(run_id)
        return None
    
    def _load_initial_theme(self):
        """
        ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ config.jsonì—ì„œ í…Œë§ˆ ì„¤ì •ì„ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        config_path = 'config.json'
        theme_to_load = 'light'  # ê¸°ë³¸ê°’
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    config = json.load(f)
                    theme_to_load = config.get('theme', 'light')
            except (json.JSONDecodeError, KeyError):
                print("Warning: Could not read theme from config.json. Defaulting to 'light'.")
                pass
        self.current_theme = theme_to_load

    # ğŸ’¥ ë³€ê²½ì  1: í•˜ì´ë¼ì´íŠ¸ ê·œì¹™ ê´€ë¦¬ ë©”ì†Œë“œë¥¼ ì»¨íŠ¸ë¡¤ëŸ¬ì— ì¶”ê°€
    def _load_highlighting_rules(self):
        """í•˜ì´ë¼ì´íŠ¸ ê·œì¹™ì„ íŒŒì¼ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤."""
        if not os.path.exists(HIGHLIGHTERS_FILE):
            return []
        try:
            with open(HIGHLIGHTERS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, Exception):
            print(f"Warning: Could not read {HIGHLIGHTERS_FILE}. Using empty rules.")
            return []

    def _save_highlighting_rules(self):
        """í˜„ì¬ í•˜ì´ë¼ì´íŠ¸ ê·œì¹™ì„ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            with open(HIGHLIGHTERS_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.highlighting_rules, f, indent=4, ensure_ascii=False)
            print("Highlighting rules saved successfully.")
        except Exception as e:
            print(f"Error saving highlighting rules: {e}")

    def get_highlighting_rules(self):
        """ë‹¤ì´ì–¼ë¡œê·¸ì— ì „ë‹¬í•  ê·œì¹™ ë°ì´í„°ì˜ ë³µì‚¬ë³¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return [dict(rule) for rule in self.highlighting_rules]
    
    def set_and_save_highlighting_rules(self, new_rules):
        """ìƒˆë¡œìš´ ê·œì¹™ì„ ì ìš©, ì €ì¥í•˜ê³  ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.highlighting_rules = new_rules
        self._save_highlighting_rules()
        self.source_model.set_highlighting_rules(self.highlighting_rules)
        print("New highlighting rules applied.")